{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OjcW0BzL7rtt"
      },
      "outputs": [],
      "source": [
        "#neccessary libraries for colab notebook to work\n",
        "!pip install swig\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ktjJse06rq9"
      },
      "outputs": [],
      "source": [
        "# @title Neccessary imports\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from matplotlib import animation\n",
        "from DQN import * \n",
        "from DDQN import * \n",
        "from DDPG import * \n",
        "from PPO import * \n",
        "from A2C import * \n",
        "from DuelingDDQN import * \n",
        "from DuelingDQN import * \n",
        "from rule_extraction import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A4hrdJiPh7IM"
      },
      "outputs": [],
      "source": [
        "# @title Create Environment\n",
        "# select discrete or continuous environment (continuous only for ddpg)\n",
        "env_continuous = False\n",
        "#create environment\n",
        "env = gym.make(\"LunarLander-v3\", continuous = env_continuous)\n",
        "#state size , action size\n",
        "print('State shape: ', env.observation_space.shape)\n",
        "print('Number of actions: ', env.action_space)\n",
        "\n",
        "#select device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "NUYa_G4v7nfl"
      },
      "outputs": [],
      "source": [
        "# @title Train Agent\n",
        "\n",
        "# pick agent to train\n",
        "\n",
        "#agent = DQNAgent(state_size = 8 , action_size = 4, device = device, hidden_sizes = [512,256],random_seed = 0, per = False, batch_norm = False)\n",
        "#scores = agent.train_agent(env, n_episodes = 2000, max_t = 500)\n",
        "\n",
        "#agent = DQNAgent(state_size = 8 , action_size = 4, device = device, hidden_sizes = [512,256],random_seed = 0, per = True, batch_norm = False)\n",
        "#scores = agent.train_agent(env, n_episodes = 2000, max_t = 500)\n",
        "\n",
        "#agent = DQNAgent(state_size = 8 , action_size = 4, device = device, hidden_sizes = [512,256],random_seed = 0, per = False, batch_norm = True)\n",
        "#scores = agent.train_agent(env, n_episodes = 2000, max_t = 500)\n",
        "\n",
        "agent = DoubleDQNAgent(state_size = 8 , action_size = 4, device = device, hidden_sizes = [256,128],random_seed = 0, per = False, batch_norm = False)\n",
        "scores = agent.train_agent(env, n_episodes = 2000, max_t = 500)\n",
        "\n",
        "#agent = DuelingDQNAgent(state_size = 8 , action_size = 4, device = device,random_seed = 0, per = False)\n",
        "#scores = agent.train_agent(env, n_episodes = 2000, max_t = 500)\n",
        "\n",
        "#agent = DuelingDQNAgent(state_size = 8 , action_size = 4, device = device,random_seed = 0, per = True)\n",
        "#scores = agent.train_agent(env, n_episodes = 2000, max_t = 500)\n",
        "\n",
        "#agent = DuelingDoubleDQNAgent(state_size = 8 , action_size = 4, device = device,random_seed = 0, per = False)\n",
        "#scores = agent.train_agent(env, n_episodes = 2000, max_t = 500)\n",
        "\n",
        "#agent = DuelingDoubleDQNAgent(state_size = 8 , action_size = 4, device = device,random_seed = 0, per = True)\n",
        "#scores = agent.train_agent(env, n_episodes = 2000, max_t = 500)\n",
        "\n",
        "#agent = A2CAgent(state_size = 8, action_size = 4, device = device)\n",
        "#scores = agent.train_agent(env, num_episodes = 2000, max_t = 500)\n",
        "\n",
        "#agent = PPOAgent(state_size = 8, action_size = 4, device = device)\n",
        "#scores = agent.train_agent(env, num_episodes = 2000, max_t = 600)\n",
        "\n",
        "#agent = DDPGAgent(state_size = 8, action_size = 2, device = device, random_seed=0)\n",
        "#scores = agent.train_agent(env, n_episodes = 2000, max_t = 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BY2LJ7khT7mE"
      },
      "outputs": [],
      "source": [
        "# @title Plot results\n",
        "\n",
        "#find running mean\n",
        "scores_window = 10\n",
        "running_mean = np.convolve(scores, np.ones(scores_window)/scores_window, mode = \"valid\")\n",
        "#plot convergence curve and running mean\n",
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.plot(np.arange(scores_window-1, len(scores)), running_mean, label='10-game Running Mean', color='red')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.grid()\n",
        "plt.savefig(\"ppo_cont\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5SYUA3h_sEyv"
      },
      "outputs": [],
      "source": [
        "# @title Watch a Smart Agent\n",
        "\n",
        "# load the trained agents from the trained_agents folder provided to the google colab terminal and watch the agent solve the environment\n",
        "\n",
        "# pick if the environment is discrete or continuous\n",
        "env_cont = False\n",
        "\n",
        "'''if env_cont = False pick one of the following agents'''\n",
        "\n",
        "#agent = DQNAgent(state_size = 8 , action_size = 4, device = device, hidden_sizes = [512,256],random_seed = 0, per = False, batch_norm = False)\n",
        "#agent.load(\"checkpoint_dqn.pth\")\n",
        "\n",
        "#agent = DQNAgent(state_size = 8 , action_size = 4, device = device, hidden_sizes = [512,256],random_seed = 0, per = True, batch_norm = False)\n",
        "#agent.load(\"checkpoint_dqn_per.pth\")\n",
        "\n",
        "#agent = DQNAgent(state_size = 8 , action_size = 4, device = device, hidden_sizes = [512,256],random_seed = 0, per = False, batch_norm = True)\n",
        "#agent.load(\"checkpoint_dqn_bn.pth\")\n",
        "\n",
        "agent = DoubleDQNAgent(state_size = 8 , action_size = 4, device = device, hidden_sizes = [256,128],random_seed = 0, per = False, batch_norm = False)\n",
        "agent.load(\"checkpoint_ddqn.pth\")\n",
        "\n",
        "#agent = DuelingDQNAgent(state_size = 8 , action_size = 4, device = device,random_seed = 0, per = False)\n",
        "#agent.load(\"checkpoint_dueling_dqn.pth\")\n",
        "\n",
        "#agent = DuelingDQNAgent(state_size = 8 , action_size = 4, device = device,random_seed = 0, per = True)\n",
        "#agent.load(\"checkpoint_dueling_dqn_per.pth\")\n",
        "\n",
        "#agent = DuelingDoubleDQNAgent(state_size = 8 , action_size = 4, device = device,random_seed = 0, per = False)\n",
        "#agent.load(\"checkpoint_dueling_ddqn.pth\")\n",
        "\n",
        "#agent = DuelingDoubleDQNAgent(state_size = 8 , action_size = 4, device = device,random_seed = 0, per = True)\n",
        "#agent.load(\"checkpoint_dueling_ddqn_per.pth\")\n",
        "\n",
        "'''if env_cont = True pick the DDPG'''\n",
        "\n",
        "#agent = DDPGAgent(state_size = 8, action_size = 2, device = device, random_seed=0)\n",
        "#agent.load(\"checkpoint_ddpg_actor.pth\", \"checkpoint_ddpg_critic.pth\")\n",
        "\n",
        "\n",
        "test_env = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = env_cont)\n",
        "frames = []\n",
        "\n",
        "state = test_env.reset()[0]\n",
        "for j in range(600):\n",
        "    frame = test_env.render()\n",
        "    frames.append(frame)\n",
        "\n",
        "    action = agent.act(state)\n",
        "    next_state, reward, terminated, truncated , _= test_env.step(action)\n",
        "    done = terminated or truncated\n",
        "    state = next_state\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "test_env.close()\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.axis('off')\n",
        "ims = [[plt.imshow(frame, animated=True)] for frame in frames]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
        "plt.close()\n",
        "\n",
        "HTML(ani.to_jshtml())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "cellView": "form",
        "id": "ix29pMNoXpXX"
      },
      "outputs": [],
      "source": [
        "# @title Load Agent\n",
        "#load agent\n",
        "\n",
        "#agent = DQNAgent(state_size = 8 , action_size = 4, device = device, hidden_sizes = [512,256],random_seed = 0, per = False, batch_norm = False)\n",
        "#agent.load(\"checkpoint_dqn.pth\")\n",
        "\n",
        "#agent = DQNAgent(state_size = 8 , action_size = 4, device = device, hidden_sizes = [512,256],random_seed = 0, per = True, batch_norm = False)\n",
        "#agent.load(\"checkpoint_dqn_per.pth\")\n",
        "\n",
        "#agent = DQNAgent(state_size = 8 , action_size = 4, device = device, hidden_sizes = [512,256],random_seed = 0, per = False, batch_norm = True)\n",
        "#agent.load(\"checkpoint_dqn_bn.pth\")\n",
        "\n",
        "#agent = DoubleDQNAgent(state_size = 8 , action_size = 4, device = device, hidden_sizes = [256,128],random_seed = 0, per = False, batch_norm = False)\n",
        "#agent.load(\"checkpoint_ddqn.pth\")\n",
        "\n",
        "#agent = DuelingDQNAgent(state_size = 8 , action_size = 4, device = device,random_seed = 0, per = False)\n",
        "#agent.load(\"checkpoint_dueling_dqn.pth\")\n",
        "\n",
        "#agent = DuelingDQNAgent(state_size = 8 , action_size = 4, device = device,random_seed = 0, per = True)\n",
        "#agent.load(\"checkpoint_dueling_dqn_per.pth\")\n",
        "\n",
        "#agent = DuelingDoubleDQNAgent(state_size = 8 , action_size = 4, device = device,random_seed = 0, per = False)\n",
        "#agent.load(\"checkpoint_dueling_ddqn.pth\")\n",
        "\n",
        "#agent = DuelingDoubleDQNAgent(state_size = 8 , action_size = 4, device = device,random_seed = 0, per = True)\n",
        "#agent.load(\"checkpoint_dueling_ddqn.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IDcltUFcox0K"
      },
      "outputs": [],
      "source": [
        "# @title Test Classifier\n",
        "tree_classifier = get_rules(env , agent, num_episodes = 200, max_depth = 4)\n",
        "mean = test_classifier(env , tree_classifier , num_of_episodes = 200)\n",
        "print(f\"Classifier : Mean score = {mean}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Y553bYVYky8R"
      },
      "outputs": [],
      "source": [
        "# @title Print Decision Tree\n",
        "print_rules(tree_classifier)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
